{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "0gCcUYLY4HcD"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/afss2/Projetos_CD/blob/main/Projeto_TAGDI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fancyimpute"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIc61WDKww3_",
        "outputId": "c8e236d3-1e2a-4a3a-8f8c-b91a272c91b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fancyimpute\n",
            "  Downloading fancyimpute-0.7.0.tar.gz (25 kB)\n",
            "Collecting knnimpute>=0.1.0\n",
            "  Downloading knnimpute-0.1.0.tar.gz (8.3 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.24.2 in /usr/local/lib/python3.7/dist-packages (from fancyimpute) (1.0.2)\n",
            "Requirement already satisfied: cvxpy in /usr/local/lib/python3.7/dist-packages (from fancyimpute) (1.2.1)\n",
            "Requirement already satisfied: cvxopt in /usr/local/lib/python3.7/dist-packages (from fancyimpute) (1.3.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from fancyimpute) (3.6.4)\n",
            "Collecting nose\n",
            "  Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
            "\u001b[K     |████████████████████████████████| 154 kB 6.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from knnimpute>=0.1.0->fancyimpute) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.7/dist-packages (from knnimpute>=0.1.0->fancyimpute) (1.21.6)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.2->fancyimpute) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.2->fancyimpute) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.2->fancyimpute) (3.1.0)\n",
            "Requirement already satisfied: osqp>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from cvxpy->fancyimpute) (0.6.2.post0)\n",
            "Requirement already satisfied: ecos>=2 in /usr/local/lib/python3.7/dist-packages (from cvxpy->fancyimpute) (2.0.10)\n",
            "Requirement already satisfied: scs>=1.1.6 in /usr/local/lib/python3.7/dist-packages (from cvxpy->fancyimpute) (3.2.0)\n",
            "Requirement already satisfied: qdldl in /usr/local/lib/python3.7/dist-packages (from osqp>=0.4.1->cvxpy->fancyimpute) (0.1.5.post2)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->fancyimpute) (1.11.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->fancyimpute) (22.1.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->fancyimpute) (0.7.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->fancyimpute) (8.14.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->fancyimpute) (1.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->fancyimpute) (57.4.0)\n",
            "Building wheels for collected packages: fancyimpute, knnimpute\n",
            "  Building wheel for fancyimpute (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fancyimpute: filename=fancyimpute-0.7.0-py3-none-any.whl size=29899 sha256=48d6790acb902f571bf6dbd10fa608d826d8010bd78ffda180a4a66c8077e4a6\n",
            "  Stored in directory: /root/.cache/pip/wheels/e3/04/06/a1a7d89ef4e631ce6268ea2d8cde04f7290651c1ff1025ce68\n",
            "  Building wheel for knnimpute (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for knnimpute: filename=knnimpute-0.1.0-py3-none-any.whl size=11353 sha256=34ef3543e09e20336269cda6b20ad3025018d46071d4937ce7dd48b955ff9fa2\n",
            "  Stored in directory: /root/.cache/pip/wheels/72/21/a8/a045cacd9838abd5643f6bfa852c0796a99d6b1494760494e0\n",
            "Successfully built fancyimpute knnimpute\n",
            "Installing collected packages: nose, knnimpute, fancyimpute\n",
            "Successfully installed fancyimpute-0.7.0 knnimpute-0.1.0 nose-1.3.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIdZg_OwZyfD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Projeto 1"
      ],
      "metadata": {
        "id": "0gCcUYLY4HcD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Coleta de dados"
      ],
      "metadata": {
        "id": "9ryOB_wE0QjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Realizando a importação do dataset dos jogadores da base de dados do FIFA 20, com 20 colunas\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "df = pd.read_csv('gdrive/MyDrive/ProjetoTAGDI/players_20.csv')\n",
        "\n",
        "df.dataframeName = 'players_20.csv'\n",
        "\n",
        "len(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5Vt5Rjpa5Ki",
        "outputId": "ac48d2e2-063c-4631-d3d2-bf87906ea7c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18278"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pré-processamento"
      ],
      "metadata": {
        "id": "W99g6J0qKIr2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definição de Tipos"
      ],
      "metadata": {
        "id": "Ne41lmw4onc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Primeiro, iremos checar os tipos e realizar a categorização dos mesmos caso não esteja correto.\n",
        "\n",
        "df.dtypes"
      ],
      "metadata": {
        "id": "Pqmfdrj-d1xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Agora imputando os novos tipos\n",
        "\n",
        "df['nationality'] = df['nationality'].astype('category')\n",
        "df['club'] = df['club'].astype('category')\n",
        "df['player_positions'] = df['player_positions'].astype('category')\n",
        "df['dob'] = df['dob'].astype('datetime64[ns]')"
      ],
      "metadata": {
        "id": "JdM2dcsted94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Substituindo no dataframe pelos códigos e salvando num dict as categorias antes da substituição, para poder consultar os valores posteriormente\n",
        "\n",
        "nat = dict(enumerate(df['nationality'].cat.categories))\n",
        "df['nationality'] = df['nationality'].cat.codes\n",
        "\n",
        "d = dict(enumerate(df['club'].cat.categories))\n",
        "df['club'] = df['club'].cat.codes"
      ],
      "metadata": {
        "id": "8mYQNn_KUs_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Após a mudança de tipos, assim ficou o dataframe\n",
        "df.dtypes"
      ],
      "metadata": {
        "id": "xlqprqj1UqHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tratamento de dados ausentes"
      ],
      "metadata": {
        "id": "wlDC2Ftjot9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Após ajustar corretamente os tipos, vamos checar se há algum dado ausente:\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "PwsgV7cTeq8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Como obtivemos exatamente 2036 jogadores com os atributos 'pace', 'shooting', 'passing', 'dribbling', 'defending' e 'physic' ausentes, suspeitamos que houvesse algo em comum entre eles.\n",
        "#Pelo nosso conhecimento do domínio, suspeitamos que se tratavam de goleiros. Testamos a hipótese e obtivemos a confirmação\n",
        "dfs = df[(df['pace'].isnull()) & (df['shooting'].isnull()) & (df['passing'].isnull()) & (df['dribbling'].isnull()) & (df['defending'].isnull()) & (df['physic'].isnull()) & (df['player_positions'] == 'GK')]\n",
        "\n",
        "print(len(dfs))\n"
      ],
      "metadata": {
        "id": "ydlJZ0PVVoIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Decidimos imputar a mediana dos valores das colunas vazias dos goleiros. Preferimos a mediana no lugar de média para reduzir a influência dos outliers\n",
        "df['pace'].fillna(df['pace'].median(), inplace=True);\n",
        "df['shooting'].fillna(df['shooting'].median(), inplace=True);\n",
        "df['passing'].fillna(df['passing'].median(), inplace=True);\n",
        "df['dribbling'].fillna(df['dribbling'].median(), inplace=True);\n",
        "df['defending'].fillna(df['defending'].median(), inplace=True);\n",
        "df['physic'].fillna(df['physic'].median(), inplace=True);\n",
        "\n"
      ],
      "metadata": {
        "id": "9H0_-vkQrN47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.isnull().sum())\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Yr9C8gVAu_MH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Vemos que a coluna com mais dados ausentes é a 'nation_position', pelo conhecimento do domínio sabemos que nem todos os jogadores atuam pela seleção de seu país. \n",
        "#Como temos muitos valores ausentes e já temos as colunas players_positions e team_position, ela acabam se tornando mais relevantes e por isso decidimos remover a coluna 'nation_position'\n",
        "df = df.drop(columns=['nation_position', 'team_position'])\n",
        "df.head()"
      ],
      "metadata": {
        "id": "AHiIV1W5dk77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aqui, fazemos mais algumas alterações no dataframe (drop das colunas short_name e player_positions) para poder utilizar o KNN para imputar os valores nulos restantes\n",
        "\n",
        "# Salvando as informações para adicionar posteriormente\n",
        "short_name_column = df['short_name'];\n",
        "player_positions_column = df['player_positions'];\n",
        "\n",
        "df = df.drop(columns=['short_name', 'player_positions']);\n",
        "\n",
        "\n",
        "df['dob'] = df['dob'].values.astype(\"float64\");"
      ],
      "metadata": {
        "id": "gxYyXWIezePt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aqui utilizamos o KNN com 3 vizinhos para realizar a imputação\n",
        "\n",
        "from fancyimpute import KNN\n",
        "fit_knn = KNN(k=3).fit_transform(df)\n",
        "\n",
        "fit_knn.shape"
      ],
      "metadata": {
        "id": "ll8c20OJw4YF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aqui fazemos a criação do dataframe utilizando os valores do KNN\n",
        "\n",
        "imputed_df = pd.DataFrame(data=fit_knn[0:,0:],columns=['age',\t'dob',\t'height_cm',\t'nationality',\t'club',\t'overall',\t'potential',\t'value_eur', 'wage_eur'\t,'release_clause_eur',\t'pace', 'shooting',\t'passing', 'dribbling', 'defending', 'physic' ]) "
      ],
      "metadata": {
        "id": "9QwszQmw3rO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# E então, vemos que não há nenhum valor nulo\n",
        "\n",
        "print(imputed_df.isnull().sum())"
      ],
      "metadata": {
        "id": "bzQCSfFu3VQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imputed_df.describe()"
      ],
      "metadata": {
        "id": "XiIaRIh7oOoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalização e discretização"
      ],
      "metadata": {
        "id": "cHdD610sozYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imputed_df.head()"
      ],
      "metadata": {
        "id": "t2DJJjueuCtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# alguns valores, como a data de nascimento (convertida para float), valor de mercado, release clause e salário acabam dominando o cálculo.\n",
        "\n",
        "dist = np.linalg.norm(imputed_df.values[1]-imputed_df.values[2])\n",
        "print(dist)"
      ],
      "metadata": {
        "id": "WjvZxIPxuTKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imputed_df_norm = (imputed_df - imputed_df.min()) / (imputed_df.max() - imputed_df.min())\n",
        "print(imputed_df_norm.head())"
      ],
      "metadata": {
        "id": "G53NJf_Dz_10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# recalculando a distância\n",
        "dist = np.linalg.norm(imputed_df_norm.values[3]-imputed_df_norm.values[4])\n",
        "print(dist)"
      ],
      "metadata": {
        "id": "8dKCH0rX0xwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Agora fazendo a discretização de algumas colunas importantes:\n",
        "\n",
        "imputed_df['age_dist'] = pd.qcut(imputed_df['age'],4)\n",
        "imputed_df['overall_dist'] = pd.qcut(imputed_df['overall'],4)\n",
        "imputed_df['potential_dist'] = pd.qcut(imputed_df['potential'],4)\n",
        "imputed_df['wage_eur_dist'] = pd.qcut(imputed_df['wage_eur'],4)\n",
        "imputed_df['value_eur_dist'] = pd.qcut(imputed_df['value_eur'],4)\n"
      ],
      "metadata": {
        "id": "Xqvo5qNdv4jD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imputed_df['overall'].describe()"
      ],
      "metadata": {
        "id": "Ka3Yy_IKyZxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imputed_df['overall_dist'].value_counts()"
      ],
      "metadata": {
        "id": "tYzFYurw5lIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Limpeza de dados"
      ],
      "metadata": {
        "id": "6TiSOcuXo3go"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Univariado)"
      ],
      "metadata": {
        "id": "kasb7aHTydXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Verificando que não possui instâncias duplicadas\n",
        "imputed_df[imputed_df.duplicated()].sort_values(\"release_clause_eur\").head()"
      ],
      "metadata": {
        "id": "GtzYNOC0SNdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imputed_df[\"release_clause_eur\"].describe()"
      ],
      "metadata": {
        "id": "P0wZ1ZFFTyJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imputed_df[\"release_clause_eur\"].plot.box()"
      ],
      "metadata": {
        "id": "QvPv0NRBT5r9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(imputed_df)"
      ],
      "metadata": {
        "id": "CNMXIb-6WS2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imputed_df[\"release_clause_eur\"].hist()"
      ],
      "metadata": {
        "id": "eTqnTKCqT_uM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import log10\n",
        "imputed_df['release_clause_eur_log'] = log10(imputed_df['release_clause_eur'])\n",
        "imputed_df['release_clause_eur_log'].hist()"
      ],
      "metadata": {
        "id": "dh5Uu0M2UDMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import abs\n",
        "mad = abs(imputed_df['release_clause_eur_log'] - imputed_df['release_clause_eur_log'].median()).median()*(1/0.6745)\n",
        "print(mad)"
      ],
      "metadata": {
        "id": "oP2PdL3EUJ3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imputed_df['release_clause_eur_log'].mad()"
      ],
      "metadata": {
        "id": "oYD4iGh5UbJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(abs(imputed_df['release_clause_eur_log']-imputed_df['release_clause_eur_log'].median())/mad).hist()"
      ],
      "metadata": {
        "id": "zdpM4-FuUgTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(imputed_df)"
      ],
      "metadata": {
        "id": "YGTUO6nEVMIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imputed_df[abs(imputed_df['release_clause_eur_log']-imputed_df['release_clause_eur_log'].median())/mad > 3.5]"
      ],
      "metadata": {
        "id": "ARESGpxMVQh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imputed_df = imputed_df[abs(imputed_df['release_clause_eur_log']-imputed_df['release_clause_eur_log'].median())/mad < 3.5]\n",
        "print(len(imputed_df))"
      ],
      "metadata": {
        "id": "g36hvMw4W34F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Bivariado)"
      ],
      "metadata": {
        "id": "9SSP-TeZxj6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imputed_df.plot.scatter(x='overall',y='release_clause_eur')"
      ],
      "metadata": {
        "id": "r63Jr_hkYP5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imputed_df['release_clause_per_overall'] = imputed_df['release_clause_eur'] / imputed_df['overall']"
      ],
      "metadata": {
        "id": "492RzKrOZiEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imputed_df['release_clause_per_overall'].describe()"
      ],
      "metadata": {
        "id": "GMeycF3cZ33s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imputed_df['release_clause_per_overall'].plot.box()"
      ],
      "metadata": {
        "id": "pTucXXWkZ8On"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imputed_df['release_clause_per_overall'].hist()"
      ],
      "metadata": {
        "id": "dty1Sm-taF6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imputed_df['release_clause_per_overall'] = log10(imputed_df['release_clause_per_overall'])"
      ],
      "metadata": {
        "id": "Uvs_T-K_aLYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imputed_df['release_clause_per_overall'].hist()"
      ],
      "metadata": {
        "id": "C8V_ysY_PgS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mad = abs(imputed_df['release_clause_per_overall'] - imputed_df['release_clause_per_overall'].median()).median()*(1/0.6745)"
      ],
      "metadata": {
        "id": "TdX3aLqvaX8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imputed_df['release_clause_per_overall'].mad()"
      ],
      "metadata": {
        "id": "oZF0ufYIdVFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "((abs(imputed_df['release_clause_per_overall']-imputed_df['release_clause_per_overall'].median()))/mad).describe()"
      ],
      "metadata": {
        "id": "qXg9-lEqb8V9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(abs(imputed_df['release_clause_per_overall']-imputed_df['release_clause_per_overall'].median())/mad).hist()"
      ],
      "metadata": {
        "id": "7LfF6gGoadCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imputed_df[abs(imputed_df['release_clause_per_overall']-imputed_df['release_clause_per_overall'].median())/mad > 2.5]"
      ],
      "metadata": {
        "id": "wonXkZPOfqva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imputed_df = imputed_df[abs(imputed_df['release_clause_per_overall']-imputed_df['release_clause_per_overall'].median())/mad < 2.5]\n",
        "print(len(imputed_df))"
      ],
      "metadata": {
        "id": "rnGgA9ipfzs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Multivariado)"
      ],
      "metadata": {
        "id": "H9tn1x4tymll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_df = imputed_df\n",
        "\n",
        "cleaned_df = cleaned_df.drop(columns=['age_dist', 'overall_dist', 'potential_dist', 'wage_eur_dist', 'value_eur_dist'])\n",
        "\n",
        "cleaned_df.head()"
      ],
      "metadata": {
        "id": "VqSY7Zgbza4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.covariance import EllipticEnvelope\n",
        "detector = EllipticEnvelope(contamination=0.01)\n",
        "detector.fit(cleaned_df)"
      ],
      "metadata": {
        "id": "qt4szHsNyoaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = detector.predict(cleaned_df)"
      ],
      "metadata": {
        "id": "7XtC-O2l0fVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores"
      ],
      "metadata": {
        "id": "BtBtea7N0hl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_df['outlier'] = scores\n",
        "print(cleaned_df.head())"
      ],
      "metadata": {
        "id": "e3loBWzt0kiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_df[cleaned_df['outlier'] == -1]"
      ],
      "metadata": {
        "id": "aczcuflu0qCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_df.head()"
      ],
      "metadata": {
        "id": "0_wMEfBR8PQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(cleaned_df)"
      ],
      "metadata": {
        "id": "MBQZYDf20xqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_df = cleaned_df[cleaned_df['outlier'] != -1]\n",
        "len(cleaned_df)"
      ],
      "metadata": {
        "id": "uKbKi0v10pgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adicionando algumas colunas novamente\n",
        "\n",
        "cleaned_df.insert(0, 'short_name', short_name_column)\n",
        "cleaned_df.insert(6, 'player_positions', player_positions_column)\n",
        "print(cleaned_df)"
      ],
      "metadata": {
        "id": "xroAz_1W7-al"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estatísticas descritivas"
      ],
      "metadata": {
        "id": "pNDwg8Wio7N1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_df['overall'].describe()"
      ],
      "metadata": {
        "id": "l0SaPMJF-EgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_df['potential'].describe()"
      ],
      "metadata": {
        "id": "XWAwKFuiC5bD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(d)"
      ],
      "metadata": {
        "id": "-5qP9v7tbvFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checar os jogadores do Real Madrid\n",
        "\n",
        "cleaned_df[cleaned_df['club'] == 504].describe()\n"
      ],
      "metadata": {
        "id": "BffWCO1wDwRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Procurando o time com maior número de jogadores\n",
        "\n",
        "cleaned_df['club'].value_counts().idxmax()\n",
        "\n"
      ],
      "metadata": {
        "id": "Vu2x8utwc_1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#O time FC Union Berlin é o que tem o maior número de jogadores (foi o primeiro a ser retornado)\n",
        "cleaned_df[cleaned_df['club'] == 317]"
      ],
      "metadata": {
        "id": "0bUkGLhcdd3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Agora vamos ver o clube que possui o maior número de jogadores com overall acima de 81.\n",
        "\n",
        "print(cleaned_df[cleaned_df['overall'] > 81]['club'].value_counts().idxmax())\n",
        "print(cleaned_df[cleaned_df['overall'] > 81][cleaned_df['club'] == 225])\n",
        "\n",
        "# O clube é o FC Bayern Munchen, com 2 jogadores"
      ],
      "metadata": {
        "id": "2F8oBijN38FF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vamos procurar o clube que possui a melhor média de jogadores que atuam como meio campo ofensivo (CAM)\n",
        "\n",
        "grouped_df1 = cleaned_df[cleaned_df['player_positions'].str.contains('CAM')].groupby(['club'])['overall'].mean()\n",
        "print(grouped_df1.idxmax())\n",
        "\n",
        "#Temos como resultado a seleção do Uruguai\n"
      ],
      "metadata": {
        "id": "k3Q6qMZx5Nyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_df.boxplot(column=['potential'])"
      ],
      "metadata": {
        "id": "aHWPd-j4GWTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_df[cleaned_df['nationality'] == 18].cov()"
      ],
      "metadata": {
        "id": "zSWAQ5uoIodt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_df[cleaned_df['nationality'] == 18].corr(method='pearson')"
      ],
      "metadata": {
        "id": "ERnTWsWbGvU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_df[cleaned_df['nationality'] == 18].corr(method='spearman')"
      ],
      "metadata": {
        "id": "W7kxoN0vLAPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_df.plot.scatter(x='overall',y='potential')"
      ],
      "metadata": {
        "id": "TeQTL4mbKb8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_df.plot.scatter(x='potential',y='release_clause_eur_log')"
      ],
      "metadata": {
        "id": "VY-kaY3IKlW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_df.plot.scatter(x='overall',y='shooting')"
      ],
      "metadata": {
        "id": "yHpLRqwSLO1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testes de hipótese"
      ],
      "metadata": {
        "id": "sYkYLKwqpAnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(nat[18])"
      ],
      "metadata": {
        "id": "_R-61PT9Mzvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_df.head()"
      ],
      "metadata": {
        "id": "CVg-Hju4ilat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_df['nationality'] "
      ],
      "metadata": {
        "id": "D5zcmfco7ygy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_df.dtypes"
      ],
      "metadata": {
        "id": "SlbiDSX-7Qwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(nat)\n",
        "codes = cleaned_df['nationality']\n",
        "print(codes)\n",
        "cleaned_df['nationality'] = codes.map(nat)\n",
        "# print(ad)"
      ],
      "metadata": {
        "id": "3j7F1spZgS1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_df.head(15)"
      ],
      "metadata": {
        "id": "M-FVTAe-GhF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_df[cleaned_df['nationality'] == 'Brazil']['release_clause_eur'].plot(kind='hist')"
      ],
      "metadata": {
        "id": "stjLOwKAr3S7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stats.shapiro(cleaned_df[cleaned_df['nationality'] == 'Brazil']['release_clause_eur'])"
      ],
      "metadata": {
        "id": "AeFfm8ytshYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_df[cleaned_df['nationality'] != 'Brazil']['release_clause_eur'].plot(kind='hist')"
      ],
      "metadata": {
        "id": "yLFVU4OzsrBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stats.shapiro(cleaned_df[cleaned_df['nationality'] != 'Brazil']['release_clause_eur'])\n",
        "#novamente percebemos que não se tratam de distribuições normais"
      ],
      "metadata": {
        "id": "-ATF2mxbstL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stats.mannwhitneyu(cleaned_df[cleaned_df['nationality'] == 'Brazil']['release_clause_eur'], cleaned_df[cleaned_df['nationality'] != 'Brazil']['release_clause_eur'], alternative ='greater')\n",
        "#Ao fazermos o teste de hipótese, percebemos que, na média, o jogador brasileiro tem uma cláusula de rescisão contrtual mais cara que a média dos jogadores estrangeiros"
      ],
      "metadata": {
        "id": "USKwWDZrs2le"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_df[cleaned_df['nationality'] == 'Brazil']['overall'].plot(kind='hist')"
      ],
      "metadata": {
        "id": "HWnQADMleTM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "stats.shapiro(cleaned_df[cleaned_df['nationality'] == 'Brazil']['overall'])"
      ],
      "metadata": {
        "id": "6W1p4yhfiRvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_df[cleaned_df['nationality'] == 'Argentina']['overall'].plot(kind='hist')"
      ],
      "metadata": {
        "id": "YLalCY_4ev0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stats.shapiro(cleaned_df[cleaned_df['nationality'] == 'Argentina']['overall'])\n",
        "#apesar dos gráfico se assemelharem a distribuições normais, percebemos que na verdade não são"
      ],
      "metadata": {
        "id": "3SOzqYwBifRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import statistics\n",
        "#verificaremos a hipótese do jogador brasileiro(18) ter, em média, um overall maior que o argentino(5)\n",
        "print(statistics.mean(cleaned_df[cleaned_df['nationality'] == 'Brazil']['overall']))\n",
        "print(statistics.mean(cleaned_df[cleaned_df['nationality'] == 'Argentina']['overall']))"
      ],
      "metadata": {
        "id": "DMrKnhzcrDel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "#Ao fazermos o teste de hipótese, percebemos que o jogador brasileiro é de fato, em média, melhor que o argentinno\n",
        "stats.mannwhitneyu(cleaned_df[cleaned_df['nationality'] == 'Brazil']['overall'], cleaned_df[cleaned_df['nationality'] == 'Argentina']['overall'], alternative ='greater')"
      ],
      "metadata": {
        "id": "rdCrl4pWf42F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Projeto 2"
      ],
      "metadata": {
        "id": "iJpvFWlH4Cif"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para essa segunda parte, iremos utilizar esse dataset para realizar o trabalho de classificar os jogadores entre 4 diferentes posições: Atacante, meio de campo, zagueiro e goleiro.\n",
        "Não iremos reutilizar o dataset com as modificações do primeiro projeto, tendo em vista que na limpeza de dados nós acabamos perdendo alguns dados importantes (na remoção de outliers)."
      ],
      "metadata": {
        "id": "OnfjvrIy4PqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Realizando a importação do dataset dos jogadores da base de dados do FIFA 20\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "raw_df = pd.read_csv('gdrive/MyDrive/ProjetoTAGDI/players_20_2.csv')\n",
        "\n",
        "raw_df.dataframeName = 'players_20_2.csv'\n",
        "\n",
        "len(raw_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfFljya-4OF3",
        "outputId": "61b27420-ffaa-4e81-b079-e69760a28a6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18278"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Escolhendo uma das colunas para realizar tarefa de classificação\n"
      ],
      "metadata": {
        "id": "O0gNP0WYupy4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora iremos mapear as posições dos jogadores de acordo com a zona do campo em que atuam. Existem 15 possíveis posições no dataset, com cada jogador podendo ter mais de uma. Mapearemos para que cada jogador tenha apenas uma posição  entre: `atacante`, `meio-campo`, `defensor` e `goleiro`"
      ],
      "metadata": {
        "id": "DZVC9obU65MB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = raw_df\n",
        "\n",
        "def combine_positions(row):\n",
        "    # Existem 15 posições diferentes e cada jogador pode possuir mais de uma posição\n",
        "    pos = row['player_positions'].split(', ') \n",
        "    N = len(pos)\n",
        "    if N < 3:\n",
        "        # Se um jogador tem 2 ou menos posições a primeira será considerada\n",
        "        pos = pos[0]\n",
        "        if pos in ['ST', 'LW', 'RW','CF']: #4\n",
        "            return 0 #Atacante\n",
        "        elif pos in ['CAM', 'LM', 'CM', 'RM', 'CDM']: #5\n",
        "            return 1 #Meio-campo\n",
        "        elif pos in ['LWB', 'RWB', 'LB', 'CB', 'RB']: #5\n",
        "            return 2 #Defensor\n",
        "        elif pos in ['GK']: #1\n",
        "            return 3 #Goleiro\n",
        "    else: # Se o jogador tem mais de 2 posições\n",
        "        position_count = [0, 0, 0, 0] \n",
        "        # Nesse for contaremos a posição que mais se repete em cada jogador, \n",
        "        # determinando a qual posição cada jogador pertence\n",
        "        for p in pos:\n",
        "            if p in ['ST', 'LW', 'RW','CF']: #4\n",
        "                index = 0 #Atacante\n",
        "            elif p in ['CAM', 'LM', 'CM', 'RM', 'CDM']: #5\n",
        "                index = 1 #Meio-campo\n",
        "            elif p in ['LWB', 'RWB', 'LB', 'CB', 'RB']: #5\n",
        "                index = 2 #Defensor\n",
        "            elif p in ['GK']: #1\n",
        "                index = 3 #Goleiro\n",
        "            else:\n",
        "                continue \n",
        "            position_count[index] += 1 \n",
        "\n",
        "        return position_count.index(max(position_count))\n",
        "\n",
        "df['player_positions'] = df.apply(combine_positions, axis=1)"
      ],
      "metadata": {
        "id": "oL0b7Vjf8v96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Modificando o dataset apenas para atributos importantes que irão ajudar na classificação:\n",
        "\n",
        "df = df[['skill_moves', 'player_positions', 'attacking_crossing', 'attacking_finishing',\n",
        "         'attacking_heading_accuracy', 'attacking_short_passing', 'attacking_volleys',\n",
        "         'skill_dribbling', 'skill_curve', 'skill_fk_accuracy', 'skill_long_passing',\n",
        "         'skill_ball_control', 'movement_acceleration', 'movement_sprint_speed', \n",
        "         'movement_agility', 'movement_reactions', 'movement_balance', 'power_shot_power',\n",
        "         'power_jumping', 'power_stamina', 'power_strength', 'power_long_shots',\n",
        "         'mentality_aggression', 'mentality_interceptions', 'mentality_positioning',\n",
        "         'mentality_vision', 'mentality_penalties', 'mentality_composure',\n",
        "         'defending_marking', 'defending_standing_tackle', 'defending_sliding_tackle',\n",
        "         'goalkeeping_diving', 'goalkeeping_handling', 'goalkeeping_kicking',\n",
        "         'goalkeeping_positioning', 'goalkeeping_reflexes']]"
      ],
      "metadata": {
        "id": "vUssqIeW8brS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verificando se existem atributos ausentes"
      ],
      "metadata": {
        "id": "IjSs4RTE7eQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "saDJ0C-68_Tp",
        "outputId": "b74669e7-47d8-4698-d596-89bb9a9b4f17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "skill_moves                   0\n",
              "player_positions              0\n",
              "attacking_crossing            0\n",
              "attacking_finishing           0\n",
              "attacking_heading_accuracy    0\n",
              "attacking_short_passing       0\n",
              "attacking_volleys             0\n",
              "skill_dribbling               0\n",
              "skill_curve                   0\n",
              "skill_fk_accuracy             0\n",
              "skill_long_passing            0\n",
              "skill_ball_control            0\n",
              "movement_acceleration         0\n",
              "movement_sprint_speed         0\n",
              "movement_agility              0\n",
              "movement_reactions            0\n",
              "movement_balance              0\n",
              "power_shot_power              0\n",
              "power_jumping                 0\n",
              "power_stamina                 0\n",
              "power_strength                0\n",
              "power_long_shots              0\n",
              "mentality_aggression          0\n",
              "mentality_interceptions       0\n",
              "mentality_positioning         0\n",
              "mentality_vision              0\n",
              "mentality_penalties           0\n",
              "mentality_composure           0\n",
              "defending_marking             0\n",
              "defending_standing_tackle     0\n",
              "defending_sliding_tackle      0\n",
              "goalkeeping_diving            0\n",
              "goalkeeping_handling          0\n",
              "goalkeeping_kicking           0\n",
              "goalkeeping_positioning       0\n",
              "goalkeeping_reflexes          0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Separando dados nos conjuntos de treinamento, validação e teste."
      ],
      "metadata": {
        "id": "-tjHUF2GuxLw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removendo a coluna `player_positions`, que é a que queremos realizar a classificação e separando os dados em conjunto de treinamento(60%), validação(20%) e teste(20%)"
      ],
      "metadata": {
        "id": "_-aWGZBw7kM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df.drop([\"player_positions\"],axis = 1)\n",
        "y = df.player_positions\n",
        "\n",
        "# Split the data to 60-20-20\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n"
      ],
      "metadata": {
        "id": "Jxf3vwJO9Wx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adicionando MLFlow + Algoritmos utilizados"
      ],
      "metadata": {
        "id": "xwbfCcsbu45G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mlflow --quiet\n",
        "!pip install pyngrok --quiet\n",
        "!pip install hyperopt --quiet"
      ],
      "metadata": {
        "id": "mQc6eYQ9oCwy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03b5f1cb-022c-4700-9644-4923a4eeffed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 16.9 MB 517 kB/s \n",
            "\u001b[K     |████████████████████████████████| 209 kB 72.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 77 kB 5.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 147 kB 75.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 181 kB 50.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 78 kB 6.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 140 kB 46.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 59 kB 5.3 MB/s \n",
            "\u001b[?25h  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 745 kB 5.1 MB/s \n",
            "\u001b[?25h  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configurando MLFLOW para rodar no google colab. Por não estarmos rodando localmente, precisamos dessas configuração para visualizarmos a UI"
      ],
      "metadata": {
        "id": "d0FvIivX7sZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "\n",
        "with mlflow.start_run(experiment_id = 1, run_name=\"MLflow on Colab\"):\n",
        "  mlflow.log_metric(\"m1\", 2.0)\n",
        "  mlflow.log_param(\"p1\", \"mlflow-colab\")\n",
        "\n",
        "get_ipython().system_raw(\"mlflow ui --port 5000 &\")\n",
        "\n",
        "from pyngrok import ngrok\n",
        "\n",
        "ngrok.kill()\n",
        "\n",
        "NGROK_AUTH_TOKEN = \"2FXkWfdo7JlGYzMgVhz2UK1esGe_7QHZE3yem9dU85ANstNXP\"\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "ngrok_tunnel = ngrok.connect(addr=\"5000\", proto=\"http\", bind_tls=True)\n",
        "print(\"MLflow Tracking UI:\", ngrok_tunnel.public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnU57sBdJA4T",
        "outputId": "be2e55a4-1a1a-47a6-8bda-ee56f9aa5ef7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLflow Tracking UI: https://5421-34-74-21-189.ngrok.io\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from hyperopt import fmin, hp, tpe, STATUS_OK\n",
        "from hyperopt.pyll import scope\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import math"
      ],
      "metadata": {
        "id": "jU8-q7VTIl2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KNN\n",
        "\n"
      ],
      "metadata": {
        "id": "lQPaNMPkHuci"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O primeiro algoritmo utilizado será o KNN, nele utilizaremos 3 hiper-parâmetros para realizar o boosting, sendo eles o número de vizinhos, o tipo de função de peso e o tipo de distância a ser utilizado."
      ],
      "metadata": {
        "id": "QHx9dIwRu_cn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(params):\n",
        "  with mlflow.start_run(experiment_id = 1, run_name='KNN'):\n",
        "    neighbors = params['n_neighbors']\n",
        "    weights = params['weights']\n",
        "    p = params['p']\n",
        "\n",
        "    neigh = KNeighborsClassifier(n_neighbors=neighbors, weights=weights, p=p)\n",
        "    neigh.fit(X_train, y_train)\n",
        "    predictions = neigh.predict(X_val)\n",
        "    score = neigh.score(X_val, y_val)\n",
        "\n",
        "    #selecionando os hiperparâmetros a serem logados pelo MLFLOW\n",
        "    mlflow.log_param(\"n_neighbors\", neighbors)\n",
        "    mlflow.log_param(\"weights\", weights)\n",
        "    mlflow.log_param(\"p\", p)\n",
        "\n",
        "    reports = classification_report(list(y_val), predictions,output_dict=True)\n",
        "    precision = reports[\"weighted avg\"][\"precision\"]\n",
        "    recall = reports[\"weighted avg\"][\"recall\"]\n",
        "    f1score = reports[\"weighted avg\"][\"f1-score\"]\n",
        "    support = reports[\"weighted avg\"][\"support\"]\n",
        "\n",
        "    #selecionando as métricas a serem logados pelo MLFLOW\n",
        "    mlflow.log_metric(\"Precision\", precision)\n",
        "    mlflow.log_metric(\"Recall\", recall)\n",
        "    mlflow.log_metric(\"F1-Score\", f1score)\n",
        "    mlflow.log_metric(\"Support\", support)\n",
        "    return {'loss': -score, 'status': STATUS_OK}\n",
        "\n",
        "search_space = { #determinando o range dos valores de hiperparâmetros a serem utlizados no treinamento\n",
        "  'n_neighbors': scope.int(hp.quniform('n_neighbors', 1, 14, q=1)),\n",
        "  'weights': hp.choice('weights', ['uniform','distance']),\n",
        "  'p': hp.choice('p', [1,2]),\n",
        "\n",
        "}\n",
        "\n",
        "algo=tpe.suggest\n",
        "\n",
        "best_hyperparameters = fmin(\n",
        "fn=train,\n",
        "space=search_space,\n",
        "algo=algo,\n",
        "max_evals=60)"
      ],
      "metadata": {
        "id": "0U1yG5c9GYqT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c14c4f8-d72e-45a7-ecb6-2c2e57bf2926"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 60/60 [05:15<00:00,  5.26s/it, best loss: -0.8864879649890591]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(best_hyperparameters)\n",
        "\n",
        "neighbors = math.floor(best_hyperparameters['n_neighbors'])\n",
        "\n",
        "# Como o hyperopt retorna os parâmetros escolhidos através do choice como a posição dele no array\n",
        "# Temos que fazer um if para mapear.\n",
        "if (best_hyperparameters['weights'] == 0):\n",
        "  weights = 'uniform'\n",
        "else:\n",
        "  weights = 'distance'\n",
        "\n",
        "if (best_hyperparameters['p'] == 0):\n",
        "  p = 1\n",
        "else:\n",
        "  p = 2\n",
        "\n",
        "best_knn = KNeighborsClassifier(n_neighbors=neighbors, weights=weights, p=p)\n",
        "best_knn.fit(X_train, y_train)\n",
        "predictions = best_knn.predict(X_test)\n",
        "score = best_knn.score(X_test, y_test)\n",
        "\n",
        "print(classification_report(list(y_test), predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06ckIPL_A5Pp",
        "outputId": "495a57d1-563b-471a-8835-012710153c50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'n_neighbors': 14.0, 'p': 0, 'weights': 1}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.76      0.81       720\n",
            "           1       0.82      0.89      0.85      1352\n",
            "           2       0.94      0.91      0.93      1189\n",
            "           3       1.00      1.00      1.00       395\n",
            "\n",
            "    accuracy                           0.88      3656\n",
            "   macro avg       0.90      0.89      0.90      3656\n",
            "weighted avg       0.88      0.88      0.88      3656\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Tree"
      ],
      "metadata": {
        "id": "6cu2qME4vK39"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O segundo algoritmo escolhido foi o Decision Tree, escolhendo também 3 hiper-parâmetros, o max-depth que é a profundidade máxima da árvore, o splitter que serve para escolher a estratégia utilizada para dividir cada nó e o criterion que é a função a ser utilizada para medir a qualidade da divisão do nó."
      ],
      "metadata": {
        "id": "jgE-ETqsvd15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(params):\n",
        "  with mlflow.start_run(experiment_id = 1, run_name='DecTree'):\n",
        "    max_depth = params['max_depth']\n",
        "    criterion = params['criterion']\n",
        "    splitter = params['splitter']\n",
        "\n",
        "    dec = DecisionTreeClassifier(max_depth=max_depth, criterion=criterion, splitter=splitter)\n",
        "    dec.fit(X_train, y_train)\n",
        "    predictions = dec.predict(X_val)\n",
        "    score = dec.score(X_val, y_val)\n",
        "\n",
        "    #selecionando os hiperparâmetros a serem logados pelo MLFLOW\n",
        "    mlflow.log_param(\"max_depth\", max_depth)\n",
        "    mlflow.log_param(\"criterion\", criterion)\n",
        "    mlflow.log_param(\"splitter\", splitter)\n",
        "\n",
        "    reports = classification_report(list(y_val), predictions,output_dict=True)\n",
        "    precision = reports[\"weighted avg\"][\"precision\"]\n",
        "    recall = reports[\"weighted avg\"][\"recall\"]\n",
        "    f1score = reports[\"weighted avg\"][\"f1-score\"]\n",
        "    support = reports[\"weighted avg\"][\"support\"]\n",
        "\n",
        "    #selecionando as métricas a serem logados pelo MLFLOW\n",
        "    mlflow.log_metric(\"Precision\", precision)\n",
        "    mlflow.log_metric(\"Recall\", recall)\n",
        "    mlflow.log_metric(\"F1-Score\", f1score)\n",
        "    mlflow.log_metric(\"Support\", support)\n",
        "    return {'loss': -score, 'status': STATUS_OK}\n",
        "\n",
        "search_space = { #determinando o range dos valores de hiperparâmetros a serem utlizados no treinamento\n",
        "  'max_depth': scope.int(hp.quniform('max_depth', 3, 15, q=1)),\n",
        "  'criterion': hp.choice('criterion', ['gini', 'entropy'],),\n",
        "  'splitter': hp.choice('splitter', ['best', 'random'],),\n",
        "\n",
        "}\n",
        "\n",
        "algo=tpe.suggest\n",
        "\n",
        "best_hyperparameters = fmin(\n",
        "fn=train,\n",
        "space=search_space,\n",
        "algo=algo,\n",
        "max_evals=60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wV2j9ye1vIWO",
        "outputId": "be9d01a9-1257-450b-fd21-e33897671b39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 60/60 [00:08<00:00,  6.82it/s, best loss: -0.8583150984682714]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(best_hyperparameters)\n",
        "\n",
        "max_depth = math.floor(best_hyperparameters['max_depth'])\n",
        "if (best_hyperparameters['criterion'] == 0):\n",
        "  criterion = 'gini'\n",
        "else:\n",
        "  criterion = 'entropy'\n",
        "\n",
        "if (best_hyperparameters['splitter'] == 0):\n",
        "  splitter = 'best'\n",
        "else:\n",
        "  splitter = 'random'\n",
        "\n",
        "best_dec = DecisionTreeClassifier(max_depth=max_depth, criterion=criterion, splitter=splitter)\n",
        "best_dec.fit(X_train, y_train)\n",
        "predictions = best_dec.predict(X_test)\n",
        "score = best_dec.score(X_test, y_test)\n",
        "\n",
        "print(classification_report(list(y_test), predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyOc-1sfDm-_",
        "outputId": "b0209f03-bd83-445a-a709-ed0579eb5e97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'criterion': 0, 'max_depth': 8.0, 'splitter': 0}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.74      0.79       720\n",
            "           1       0.78      0.84      0.81      1352\n",
            "           2       0.89      0.89      0.89      1189\n",
            "           3       1.00      1.00      1.00       395\n",
            "\n",
            "    accuracy                           0.85      3656\n",
            "   macro avg       0.88      0.87      0.87      3656\n",
            "weighted avg       0.85      0.85      0.85      3656\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVM / SVC"
      ],
      "metadata": {
        "id": "ef-4aqtXvLds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como terceiro classificador, escolhemos o SVC ou C-Support Vector Classification. Como o dataset é pequeno pudemos utilizá-lo, mas para datasets grandes ele acaba sendo impraticável e muito custoso.\n",
        "Como hiper-parâmetros, escolhemos o kernel e o C. O kernel é a função utilizada para colocar os dados no padrão de entrada e o C é o parâmetro de regularização."
      ],
      "metadata": {
        "id": "ZLzVhr0RwCkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(params):\n",
        "  with mlflow.start_run(experiment_id = 1, run_name='SVM'):\n",
        "    kernel = params['kernel']\n",
        "    C = params['C']\n",
        "\n",
        "    #selecionando os hiperparâmetros a serem logados pelo MLFLOW\n",
        "    mlflow.log_param(\"kernel\", kernel)\n",
        "    mlflow.log_param(\"C\", C)\n",
        "\n",
        "    svm = SVC(kernel=kernel, C=C)\n",
        "    svm.fit(X_train, y_train)\n",
        "\n",
        "    predictions = svm.predict(X_val)\n",
        "    score = svm.score(X_val, y_val)\n",
        "\n",
        "    reports = classification_report(list(y_val), predictions,output_dict=True)\n",
        "    precision = reports[\"weighted avg\"][\"precision\"]\n",
        "    recall = reports[\"weighted avg\"][\"recall\"]\n",
        "    f1score = reports[\"weighted avg\"][\"f1-score\"]\n",
        "    support = reports[\"weighted avg\"][\"support\"]\n",
        "\n",
        "    #selecionando as métricas a serem logados pelo MLFLOW\n",
        "    mlflow.log_metric(\"Precision\", precision)\n",
        "    mlflow.log_metric(\"Recall\", recall)\n",
        "    mlflow.log_metric(\"F1-Score\", f1score)\n",
        "    mlflow.log_metric(\"Support\", support)\n",
        "    return {'loss': -score, 'status': STATUS_OK}\n",
        "\n",
        "search_space = { #determinando o range dos valores de hiperparâmetros a serem utlizados no treinamento\n",
        "  'C': hp.choice('C', np.logspace(-2, 3, 13)),\n",
        "  'kernel': hp.choice('kernel', ['rbf', 'sigmoid']),\n",
        "}\n",
        "\n",
        "algo=tpe.suggest\n",
        "\n",
        "best_hyperparameters = fmin(\n",
        "fn=train,\n",
        "space=search_space,\n",
        "algo=algo,\n",
        "max_evals=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UW8yrunvI5S",
        "outputId": "35f7d8dd-b34c-48ce-8ac2-2eb360e9fc5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  2%|▏         | 1/50 [00:21<17:23, 21.29s/it, best loss: -0.3525711159737418]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 12%|█▏        | 6/50 [01:17<10:18, 14.06s/it, best loss: -0.8925054704595186]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 22%|██▏       | 11/50 [02:28<09:37, 14.81s/it, best loss: -0.899343544857768]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 74%|███████▍  | 37/50 [05:30<02:26, 11.29s/it, best loss: -0.9026258205689278]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 88%|████████▊ | 44/50 [06:26<01:04, 10.80s/it, best loss: -0.9026258205689278]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 94%|█████████▍| 47/50 [06:56<00:35, 11.79s/it, best loss: -0.9026258205689278]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 50/50 [07:22<00:00,  8.85s/it, best loss: -0.9026258205689278]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(best_hyperparameters)\n",
        "\n",
        "C = math.floor(best_hyperparameters['C'])\n",
        "if (best_hyperparameters['kernel'] == 0):\n",
        "  kernel = 'rbf'\n",
        "else:\n",
        "  kernel = 'sigmoid'\n",
        "\n",
        "best_svm = SVC(kernel=kernel, C=C)\n",
        "best_svm.fit(X_train, y_train)\n",
        "\n",
        "predictions = best_svm.predict(X_test)\n",
        "score = best_svm.score(X_test, y_test)\n",
        "  \n",
        "print(classification_report(list(y_test), predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAbGLeHvEll_",
        "outputId": "0ded1679-4e36-4ae2-c806-9fa86a1a78cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'C': 7, 'kernel': 0}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.76      0.82       720\n",
            "           1       0.83      0.90      0.86      1352\n",
            "           2       0.94      0.94      0.94      1189\n",
            "           3       1.00      1.00      1.00       395\n",
            "\n",
            "    accuracy                           0.89      3656\n",
            "   macro avg       0.91      0.90      0.90      3656\n",
            "weighted avg       0.90      0.89      0.89      3656\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regressão Logística"
      ],
      "metadata": {
        "id": "_4DYoFqavL2w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como quarto classificador, utilizamos a regressão logística (que também pode ser utilizada em problemas de classificação) e como hiper-parâmetros, escolhemos o solver e o C novamente. O solver é o algoritmo utilizado para o problema de otimização e o C é o inverso da força de regularização."
      ],
      "metadata": {
        "id": "m4wbq6rpw17J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(params):\n",
        "  with mlflow.start_run(experiment_id = 1, run_name='REGL'):\n",
        "    solver = params['solver']\n",
        "    C = params['C']\n",
        "\n",
        "    lr = LogisticRegression(solver=solver, C=C, max_iter = 200)\n",
        "    lr.fit(X_train, y_train)\n",
        "\n",
        "    predictions = lr.predict(X_val)\n",
        "    score = lr.score(X_val, y_val)\n",
        "\n",
        "    #selecionando os hiperparâmetros a serem logados pelo MLFLOW\n",
        "    mlflow.log_param(\"solver\", solver)\n",
        "    mlflow.log_param(\"C\", C)\n",
        "\n",
        "    reports = classification_report(list(y_val), predictions,output_dict=True)\n",
        "    precision = reports[\"weighted avg\"][\"precision\"]\n",
        "    recall = reports[\"weighted avg\"][\"recall\"]\n",
        "    f1score = reports[\"weighted avg\"][\"f1-score\"]\n",
        "    support = reports[\"weighted avg\"][\"support\"]\n",
        "\n",
        "    #selecionando as métricas a serem logados pelo MLFLOW\n",
        "    mlflow.log_metric(\"Precision\", precision)\n",
        "    mlflow.log_metric(\"Recall\", recall)\n",
        "    mlflow.log_metric(\"F1-Score\", f1score)\n",
        "    mlflow.log_metric(\"Support\", support)\n",
        "    return {'loss': -score, 'status': STATUS_OK}\n",
        "\n",
        "search_space = { #determinando o range dos valores de hiperparâmetros a serem utlizados no treinamento\n",
        "  'C': hp.choice('C', np.logspace(-2, 3, 13)),\n",
        "  'solver': hp.choice('solver', ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']),\n",
        "}\n",
        "\n",
        "algo=tpe.suggest\n",
        "\n",
        "best_hyperparameters = fmin(\n",
        "fn=train,\n",
        "space=search_space,\n",
        "algo=algo,\n",
        "max_evals=50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmLg5u1NvJZ7",
        "outputId": "8e261f9f-a349-42a3-bbf5-32a920c33746"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  2%|▏         | 1/50 [00:02<02:10,  2.66s/it, best loss: -0.888129102844639]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r  4%|▍         | 2/50 [00:05<02:18,  2.88s/it, best loss: -0.8886761487964989]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r  6%|▌         | 3/50 [00:07<02:01,  2.58s/it, best loss: -0.8886761487964989]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r  8%|▊         | 4/50 [00:10<01:59,  2.61s/it, best loss: -0.8886761487964989]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 12%|█▏        | 6/50 [00:13<01:29,  2.04s/it, best loss: -0.8886761487964989]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 14%|█▍        | 7/50 [00:15<01:29,  2.08s/it, best loss: -0.8886761487964989]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 16%|█▌        | 8/50 [00:18<01:40,  2.40s/it, best loss: -0.8886761487964989]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 18%|█▊        | 9/50 [00:20<01:34,  2.31s/it, best loss: -0.8886761487964989]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 20%|██        | 10/50 [00:23<01:30,  2.26s/it, best loss: -0.8886761487964989]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 22%|██▏       | 11/50 [00:25<01:33,  2.39s/it, best loss: -0.8886761487964989]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 24%|██▍       | 12/50 [00:28<01:37,  2.58s/it, best loss: -0.8886761487964989]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 26%|██▌       | 13/50 [00:31<01:40,  2.72s/it, best loss: -0.8886761487964989]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 28%|██▊       | 14/50 [00:34<01:36,  2.69s/it, best loss: -0.8886761487964989]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 34%|███▍      | 17/50 [00:46<01:46,  3.23s/it, best loss: -0.8886761487964989]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 36%|███▌      | 18/50 [00:49<01:37,  3.05s/it, best loss: -0.8886761487964989]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 42%|████▏     | 21/50 [01:10<02:32,  5.27s/it, best loss: -0.8886761487964989]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 44%|████▍     | 22/50 [01:13<02:08,  4.60s/it, best loss: -0.8886761487964989]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 46%|████▌     | 23/50 [01:16<01:51,  4.12s/it, best loss: -0.8886761487964989]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 48%|████▊     | 24/50 [01:19<01:38,  3.80s/it, best loss: -0.8886761487964989]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 50%|█████     | 25/50 [01:22<01:29,  3.57s/it, best loss: -0.8886761487964989]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 52%|█████▏    | 26/50 [01:25<01:21,  3.41s/it, best loss: -0.8886761487964989]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 56%|█████▌    | 28/50 [01:29<00:59,  2.71s/it, best loss: -0.8886761487964989]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 58%|█████▊    | 29/50 [01:32<00:58,  2.80s/it, best loss: -0.8886761487964989]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 60%|██████    | 30/50 [01:35<00:57,  2.88s/it, best loss: -0.8886761487964989]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 62%|██████▏   | 31/50 [02:35<06:17, 19.88s/it, best loss: -0.8886761487964989]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 66%|██████▌   | 33/50 [02:38<03:03, 10.82s/it, best loss: -0.8886761487964989]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 68%|██████▊   | 34/50 [02:42<02:15,  8.49s/it, best loss: -0.8886761487964989]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 70%|███████   | 35/50 [02:44<01:40,  6.72s/it, best loss: -0.8886761487964989]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 74%|███████▍  | 37/50 [02:47<00:53,  4.11s/it, best loss: -0.8886761487964989]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 76%|███████▌  | 38/50 [02:49<00:42,  3.53s/it, best loss: -0.8886761487964989]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 78%|███████▊  | 39/50 [02:52<00:37,  3.38s/it, best loss: -0.8886761487964989]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 80%|████████  | 40/50 [02:55<00:33,  3.32s/it, best loss: -0.8886761487964989]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 82%|████████▏ | 41/50 [03:48<02:42, 18.07s/it, best loss: -0.8886761487964989]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 84%|████████▍ | 42/50 [03:51<01:47, 13.43s/it, best loss: -0.8886761487964989]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 86%|████████▌ | 43/50 [03:54<01:12, 10.31s/it, best loss: -0.8886761487964989]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 88%|████████▊ | 44/50 [03:56<00:47,  7.84s/it, best loss: -0.8886761487964989]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 92%|█████████▏| 46/50 [04:00<00:19,  4.92s/it, best loss: -0.8886761487964989]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 94%|█████████▍| 47/50 [04:02<00:12,  4.23s/it, best loss: -0.8886761487964989]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 96%|█████████▌| 48/50 [04:05<00:07,  3.88s/it, best loss: -0.8886761487964989]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 50/50 [04:16<00:00,  5.13s/it, best loss: -0.8886761487964989]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(best_hyperparameters)\n",
        "\n",
        "C = math.floor(best_hyperparameters['C'])\n",
        "if (best_hyperparameters['solver'] == 0):\n",
        "  solver = 'liblinear'\n",
        "elif (best_hyperparameters['solver'] == 1):\n",
        "  solver = 'newton-cg'\n",
        "elif (best_hyperparameters['solver'] == 2):\n",
        "  solver = 'lbfgs'\n",
        "elif (best_hyperparameters['solver'] == 3):\n",
        "  solver = 'sag'\n",
        "elif (best_hyperparameters['solver'] == 4):\n",
        "  solver = 'saga'\n",
        "\n",
        "best_lr = LogisticRegression(solver=solver, C=C, max_iter = 200)\n",
        "best_lr.fit(X_train, y_train)\n",
        "\n",
        "predictions = best_lr.predict(X_test)\n",
        "score = best_lr.score(X_test, y_test)\n",
        "  \n",
        "print(classification_report(list(y_test), predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqhWpGbyE4k-",
        "outputId": "33ff5bec-6d43-4661-da58-f666f3125d85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'C': 7, 'solver': 4}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.80      0.83       720\n",
            "           1       0.83      0.87      0.85      1352\n",
            "           2       0.92      0.92      0.92      1189\n",
            "           3       1.00      1.00      1.00       395\n",
            "\n",
            "    accuracy                           0.88      3656\n",
            "   macro avg       0.90      0.90      0.90      3656\n",
            "weighted avg       0.89      0.88      0.88      3656\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparação entre os classificadores/Diagnóstico"
      ],
      "metadata": {
        "id": "7JLC0VA9ojab"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analisando todos os classificadores utilizando seus melhores hiperparâmetros, tivemos os seguindos resultados:\n",
        "\n",
        "Respectivamente precision, recall, f1-score e número total de amostras:\n",
        "\n",
        "*   KNN: \n",
        "  * 0.88\n",
        "  * 0.88\n",
        "  * 0.88\n",
        "  * 3656\n",
        "* Decision Tree: \n",
        "  * 0.85 \n",
        "  * 0.85 \n",
        "  * 0.85 \n",
        "  * 3656\n",
        "* SVC: \n",
        "  * 0.90\n",
        "  * 0.89\n",
        "  * 0.89      \n",
        "  * 3656\n",
        "* Regressão Logistica:     \n",
        "  * 0.89\n",
        "  * 0.88\n",
        "  * 0.88\n",
        "  * 3656\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "beJisPxcrCGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Desses modelos, o que mais se mostrou eficiente foi o SVC, com uma leve vantagem quanto a regressão logística (1% na frente em todas as métricas) e KNN (2% em accuracy e 1% no restante).\n",
        "\n",
        "O SVC procura um hiperplano que melhor divide os dados, procurando a possível melhor margem (distância) entre a observação (vetor de suporte) e o hiperplano.\n",
        "\n",
        "Ele reduz o problema em múltiplos problemas de binários de classificação (1 x 1), por exemplo:\n",
        "\n",
        "Atacante x Meio Campo\n",
        "Atacante x Goleiro\n",
        "Meio Campo x Goleiro...\n",
        "\n",
        "\n",
        "O pior modelo foi a decision tree, que ficou 4~5% atrás do SVC.\n",
        "\n",
        "Levando em conta o tempo de treinamento, os classificadores SVC e de regressão logística demoraram bem mais, algo que pode ser percebido através do MLFlow.\n",
        "\n",
        "Em conclusão, mesmo o modelo mais ineficiente ainda tem uma boa precisão, o KNN também poderia ser utilizado caso tivessemos um dataset ainda maior e que o tempo de processamento fosse um parâmetro de escolha do classificador, tendo em vista que ainda possui resultados muito bons."
      ],
      "metadata": {
        "id": "KYW7ydijsoiX"
      }
    }
  ]
}